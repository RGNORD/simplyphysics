<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/simplyphysics/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/simplyphysics/" rel="alternate" type="text/html" /><updated>2024-02-05T21:47:06+02:00</updated><id>http://localhost:4000/simplyphysics/feed.xml</id><title type="html">Simply Physics</title><subtitle>Physics ramblings by an alleged figure skater.</subtitle><entry><title type="html">Matrix Tricks in QM</title><link href="http://localhost:4000/simplyphysics/general/2024/04/01/qm-matrix-tricks.html" rel="alternate" type="text/html" title="Matrix Tricks in QM" /><published>2024-04-01T00:00:00+03:00</published><updated>2024-04-01T00:00:00+03:00</updated><id>http://localhost:4000/simplyphysics/general/2024/04/01/qm-matrix-tricks</id><content type="html" xml:base="http://localhost:4000/simplyphysics/general/2024/04/01/qm-matrix-tricks.html">&lt;p&gt;In a previous article on this site (“A Gentle Introduction to Computational Quantum Mechanics”) we derived a matrix form for the
Hamiltonian in the time-independent Schrödinger equation. Let’s solve
that Hamiltonian analytically. No Numpy needed for the special case of a
particle in a box!&lt;/p&gt;

&lt;p&gt;The standard textbook answer for the eigenenergies of a particle in a
box is&lt;/p&gt;

\[\begin{align}
    E_n = \frac{n^2\pi ^2}{2L^2}.\end{align}\]

&lt;p&gt;Let’s try to see if we can figure this out from the matrix form&lt;/p&gt;

\[\begin{align}
    D = -\frac{1}{2(\Delta x)^2} \begin{bmatrix}
        -2 &amp;amp; 1 &amp;amp; 0 &amp;amp;  \cdots \\
        1 &amp;amp; -2 &amp;amp; 1 &amp;amp;  \ddots  \\
        0 &amp;amp; 1 &amp;amp; -2 &amp;amp;  \ddots \\
        \vdots &amp;amp; \ddots &amp;amp; \ddots &amp;amp; \ddots 
    \end{bmatrix}.\end{align}\]

&lt;p&gt;It turns out that this sort of matrix is called a &quot;tridiagonal Toeplitz
matrix&quot;. We can simply take the eigenvalues from the literature; they
are&lt;/p&gt;

\[\begin{align}
    E_n=a-2\sqrt{bc} \cos \bigg(\frac{n \pi }{N+1}\bigg)\end{align}\]

&lt;p&gt;where $a$ are the diagonal entries, $b$ and $c$ are the offdiagonal
entries. In our case, $a=1/(\Delta x)^2$ and $b=c=-0.5/(\Delta x)^2$ and
$N\cdot N$ is the size of the matrix.&lt;/p&gt;

&lt;p&gt;Presuming $N\gg 1$, we can approximate $N+1\approx N$ and take the
Taylor series of $\cos$, to wit&lt;/p&gt;

\[\begin{align}
    \cos \bigg( \frac{n\pi }{N+1} \bigg) \approx \cos \bigg( \frac{n\pi }{N} \bigg) \approx 1 - \frac{n^2 \pi ^2 }{2N^2}.\end{align}\]

&lt;p&gt;Now, since our matrix comes from discretizing the space in to N+1
intervals with N gridpoints, evidently $N = L/\Delta x$. Hence&lt;/p&gt;

\[\begin{align}
    1 - \frac{n^2 \pi ^2 }{2N^2} = 1 - \frac{n^2 \pi ^2 (\Delta x)^2 }{2L^2}.\label{eq:finalcos}\end{align}\]

&lt;p&gt;Plugging in $\eqref{eq:finalcos}$ and the values for $a,b$ and $c$, we
get&lt;/p&gt;

\[\begin{align}
    E_n =\frac{1}{(\Delta x)^2} - \frac{1}{(\Delta x)^2}\bigg( 1-\frac{n^2 \pi ^2 (\Delta x)^2 }{2L^2}\bigg) = \frac{n^2\pi ^2 }{2L ^2}\end{align}\]

&lt;p&gt;which is precisely what we would get by standard methods.&lt;/p&gt;

&lt;p&gt;Well, you say, it’s not a very impressive trick – this is a very
special form of matrix. If we added a potential, then all the entries on
the diagonal wouldn’t be the same and we wouldn’t get such a pretty
result.&lt;/p&gt;

&lt;p&gt;True! There is one more exception: if we add 1 to the top right and
bottom left corners – indicating periodic boundary conditions – the
matrix is still solvable (it is now a circulant matrix). It is also
possible to apply the perturbation theory of matrices&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; to get more
analytical results, even for more unusual matrix types.&lt;/p&gt;

&lt;p&gt;This technique is not very useful in practice, but it was a funny find
anyway. If you find more quantum systems solvable in this way, I would
be interested – contact me.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kato’s book &quot;Perturbation theory of linear operators&quot; is the
definitive resource for this. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="general" /><category term="quantum mechanics" /><summary type="html">In a previous article on this site (“A Gentle Introduction to Computational Quantum Mechanics”) we derived a matrix form for the Hamiltonian in the time-independent Schrödinger equation. Let’s solve that Hamiltonian analytically. No Numpy needed for the special case of a particle in a box!</summary></entry><entry><title type="html">Scaling in the Ising Model</title><link href="http://localhost:4000/simplyphysics/general/2024/03/16/ising-model-scaling.html" rel="alternate" type="text/html" title="Scaling in the Ising Model" /><published>2024-03-16T00:00:00+02:00</published><updated>2024-03-16T00:00:00+02:00</updated><id>http://localhost:4000/simplyphysics/general/2024/03/16/ising-model-scaling</id><content type="html" xml:base="http://localhost:4000/simplyphysics/general/2024/03/16/ising-model-scaling.html">&lt;p&gt;This is a first introduction to numerical renormalization methods. In
terms of the code, I followed the tutorial in&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;basics-of-the-ising-model&quot;&gt;Basics of the Ising Model&lt;/h2&gt;

&lt;p&gt;The Ising model is a well-studied and simple theory for magnetization.
The basic idea of the model is to consider a grid (in our case, either
1D chain or 2D grid) of spins, some of which &quot;point up&quot; (and thus have
value +1) and some of which &quot;point down&quot; (have value -1). The energy
of the system is then&lt;/p&gt;

\[\begin{align}
    E = -J\sum _{\langle ij\rangle} s_is_j \label{eq:energyising}\end{align}\]

&lt;p&gt;with $\langle ij\rangle$ indicating nearest-neighbor sums. So, it is a
very simple thing to compute: just go over each of the spins, and take
products with its (non-diagonal) neighbors.&lt;/p&gt;

&lt;p&gt;The code used for the graph in this text is just a basic Ising Monte
Carlo code; since the model is a staple of physics teachers everywhere,
it is easy to find tutorials on how to make your own. I won’t spend time
on it in this article.&lt;/p&gt;

&lt;h2 id=&quot;scaling&quot;&gt;Scaling&lt;/h2&gt;

&lt;p&gt;The basic question of scaling (renormalization) methods is this: what
happens if you zoom out of the system?&lt;/p&gt;

&lt;p&gt;By zooming out, I mean the following. Take a grid containing, say,
$9\cdot 9$ cells, with some arrangement of spins. What if we treated
this grid in some averaged fashion to save some work?&lt;/p&gt;

&lt;p&gt;This question pops up quite frequently in two places: particle physics
and drinking in a bar. In particle physics, things like masses of
particles going in to the equations are different depending on what
energy scale you’re looking at. Calculations with &quot;bare&quot; masses tend
to produce infinite results that have to be removed by basically
inserting some energy-dependence. As for the bars, the attractiveness of
your gender of choice tends to be directly correlated with the amount of
alcohol you’ve consumed. Beware – a renormalized attractiveness scale
may lead to regret in the morning (the equivalent situation in particle
physics rarely causes distress).&lt;/p&gt;

&lt;p&gt;Obviously, the dependence of our quantity of interest on $J$ in
$\eqref{eq:energyising}$ doesn’t have to stay the same in this &quot;zooming
out&quot; process. We must work out some way to find a function, say $R(J)$,
which takes in a coupling constant and returns the coupling constant for
a coarser model which keeps the magnetization unchanged.&lt;/p&gt;

&lt;p&gt;How could we find this function? Well, first we must get the
magnetization for the large grid, $9 \cdot 9$. Then we average over this
grid somehow. For example, we could divide it in to blocks of size
$3\cdot 3$, and have the spins in those blocks &quot;vote&quot;: if there are
more up-spins than down-spins, the averaged block has an up spin, and
vice versa. We now compute this &quot;coarse-grained&quot; (CG) magnetization.
Then, we compare it to a native 3x3 grid, one where we didn’t average
over the cells, but rather just did the calculation in the 3x3 system
straight away. Here’s a picture of coarse graining:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/coarse-graining.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To recap, at this point we have two functions:
$\langle M^2&lt;em&gt;{3\cdot 3}(J) \rangle$ and
$\langle (M&lt;/em&gt;{3\cdot 3}^{CG }(J))^2\rangle$. But these two are not
necessarily the same: coarse-grained magnetization probably doesn’t
match the native 3x3 calculation.&lt;/p&gt;

&lt;p&gt;If we want to see how to change J to get the same results in both
methods, we have to find a function such that:&lt;/p&gt;

\[\begin{aligned}
    \langle (M_{9\cdot 9}^{CG }(J))^2\rangle = \langle M_{9\cdot 9}^2(J&apos;)\rangle, \quad J&apos;=R(J)\end{aligned}\]

&lt;p&gt;So the sensible thing to do is to plot these two curves in the same
plot, and try to work out the function $R(J)$ from there. We can see
what the function is just visually by looking at the picture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/magnetization_renorm.png&quot; alt=&quot;Magnetization for a coarse-grained and native grid. The intersection
point indicates a fixed point.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that in the picture above.&lt;/p&gt;

&lt;p&gt;What is the meaning of the point at which the lines cross? At those
points, evidently, the coupling constants are same at both scales for
the same magnetization. Remember that the calculation only depends on
$\beta J=J/T$, not $\beta$ and $J$ separately; changing one is
equivalent to changing the other. So at approximately the point
$T = 2.3$, it doesn’t matter at which scale we look at the problem,
coarse grained or not.&lt;/p&gt;

&lt;p&gt;One of Ken Wilson’s great triumphs was understanding that these fixed
points denote phase changes. In fact, a characteristic property of phase
changes is that &lt;em&gt;they depend on all scales&lt;/em&gt;. When the magnet is changing
from a ferromagnet to non-magnetic, it is not merely a local,
small-scale phenomenon, nor is it merely a large-scale aggregate issue.
When the phase change happens, it happens at every scale simultaneously;
all scales, both long-range and short-range, high-energy and low-energy,
contribute to the system. It is precisely this property that makes phase
change so dramatic.&lt;/p&gt;

&lt;p&gt;The relationship of this to renormalization in QFT is that it’s
basically the same process. The masses and coupling constants depend on
the scale. The method I’ve used in this post is just an extremely clumsy
and bad way to figure out how things change at scale and where the phase
transitions are, but there are better ones. I will look at one of them
in the Part 2 of this series (stay tuned..).&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://clark.physics.illinois.edu/html/Ising/IsingModel.html&quot;&gt;https://clark.physics.illinois.edu/html/Ising/IsingModel.html&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="general" /><category term="quantum field theory" /><summary type="html">This is a first introduction to numerical renormalization methods. In terms of the code, I followed the tutorial in1. https://clark.physics.illinois.edu/html/Ising/IsingModel.html &amp;#8617;</summary></entry><entry><title type="html">Baby’s First Philosophy of Quantum Field Theory Article</title><link href="http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/10/13/taking-qft-seriously.html" rel="alternate" type="text/html" title="Baby’s First Philosophy of Quantum Field Theory Article" /><published>2023-10-13T00:00:00+03:00</published><updated>2023-10-13T00:00:00+03:00</updated><id>http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/10/13/taking-qft-seriously</id><content type="html" xml:base="http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/10/13/taking-qft-seriously.html">&lt;p&gt;If you’re even remotely interested in the philosophy of physics, you’ve
surely heard of quantum mechanics. Hell, nowadays quantum mechanics is
even in your washing machine – who among us would use non-quantum
soap?&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; No doubt you’ve heard all sorts of wonderful things about
quantum particles instantly knowing what happens to their entangled
partner, or being in two places at once, and so on.&lt;/p&gt;

&lt;p&gt;Most of the things philosophers have talked about are essentially
features of non-relativistic quantum mechanics; that is, the quantum
mechanics of particles moving fairly slowly compared to the speed of
light. Low-energy stuff. That’s understandable for two reasons. Firstly,
most of the philosophically interesting features of quantum mechanics
are already present in the non-relativistic version (and don’t by any
means disappear in the relativistic version), so there’s plenty to
discuss. Second, a lot of stuff in the real world is mostly dependent on
these low-energy phenomena, such as the properties of materials,
semiconductors and so on.&lt;/p&gt;

&lt;p&gt;Nevertheless, there are legitimate reasons to (philosophically, morally,
aesthetically, drunkenly) care about relativistic quantum mechanics –
quantum field theory (QFT). Plenty of ordinary phenomena are &lt;em&gt;not&lt;/em&gt;
low-energy. For example, the sky is blue because of Rayleigh scattering,
which is a relativistic phenomenon. So you need QFT – and it turns out
that QFT not only inherits the normal quantum problems but adds new ones
on top. This is an introduction to those issues.&lt;/p&gt;

&lt;p&gt;I’ve written this article centering around two philosophers of QFT,
Doreen Fraser and David Wallace. They take opposing viewpoints on
important issues in the philosophy of QFT. I’ll admit my bias upfront –
I think Wallace is right, Fraser wrong. I’ll try to give it a fair shake
anyway, and regardless, this text is mostly intended as an introduction
to the issues, not the particular argument.&lt;/p&gt;

&lt;h1 id=&quot;basic-argument&quot;&gt;Basic Argument&lt;/h1&gt;

&lt;p&gt;I will start by outlining the arguments in the papers in very general
terms, skipping over details and technicalities. This has the advantage
of immediately naming a host of important issues in QFT, which I’ll then
deal with in turn in later sections.&lt;/p&gt;

&lt;p&gt;Quantum field theory is more of a mathematical framework than a
particular theory. The issue at hand is which version of QFT to use for
foundational discussions about philosophy. Fraser believes we should use
a QFT called &quot;algebraic quantum field theory&quot; (AQFT), a mathematically
sophisticated version, whereas Wallace advocates for using
&quot;conventional quantum field theory&quot; (CQFT), the theory practicing
physicists actually do practical calculations in. Below, I’ve emphasized
the words that I will explain in later sections.&lt;/p&gt;

&lt;p&gt;Fraseer’s argument against using CQFT runs basically as follows. CQFT,
as a theory, is mathematically ill-defined. If we use it in a naive
fashion, the calculations end up giving infinite answers and running in
to &lt;em&gt;Haag’s theorem&lt;/em&gt; which basically proves the theory is inconsistent.
We can only cure the infinities and avoid Haag’s theorem in a dubious
fashion (&lt;em&gt;renormalization&lt;/em&gt;), which causes the theory to lose its
&lt;em&gt;Lorentz-invariance&lt;/em&gt;. A theory this poorly defined can’t be suited to
foundational work; rather, we should use AQFT, which has a
mathematically sound definition and in which none of the above issues
apply. And, after all, though AQFT is less-developed than CQFT, we
already have examples of AQFTs in 2-dimensional spacetimes, and they’re
equivalent to the same theory given in the CQFT formalism, so we should
use the mathematically well-defined theory instead of an ill-defined
mess. We have a genuine case of &lt;em&gt;underdetermination of theory by
evidence&lt;/em&gt; and must choose the foundationally sound theory.&lt;/p&gt;

&lt;p&gt;Wallace’s counter-argument points out that while 2-dimensional examples
of AQFT exist, there exist no 4-dimensional interacting AQFTs. And, as
you may have noticed, we live in a 4-dimensional world (three spatial
dimensions and time). Therefore, it’s not currently possible to
construct the Standard Model of particle physics in the AQFT framework.
You can, on the other hand, construct the Standard Model in CQFT just
fine. Why would you use AQFT for foundational work when it can’t even
construct the most succesful scientific theory we have? We should
instead prefer CQFT, which is capable of the Standard Model. Wallace
argues also that while renormalization may have been a dubious procedure
in the 1950s, it is now a mathematically well-understood; any problems
it causes with Lorentz-invariance have been made basically irrelevant by
progress in renormalization theory.&lt;/p&gt;

&lt;p&gt;Well, that was a mouthful, wasn’t it? Unless you’re already familiar
with QFT, you probably didn’t understand most of it. Let’s fix that.&lt;/p&gt;

&lt;h1 id=&quot;unitary-inequivalence-and-haags-theorem&quot;&gt;Unitary Inequivalence and Haag’s Theorem&lt;/h1&gt;

&lt;p&gt;Haag’s theorem is a brilliant result that demonstrates the inconsistency
of quantum field theory – it basically shows the whole theory (in its
CQFT formulation) is mathematically incoherent if applied naively.
Here’s how it works.&lt;/p&gt;

&lt;p&gt;Haag wrote down a bunch of obvious-looking assumptions about QFT. For
example, he assumed it’s possible to uniquely define a state with no
particles (duh - obviously you know what &quot;no particles&quot; means), and
that the theory is &lt;em&gt;Lorentz-invariant&lt;/em&gt;, that is to say, it obeys
Einstein’s special relativity (speed of light is the maximum speed). All
of the assumptions he made were a standard part of QFT. He then
rug-pulled his audience by showing that they lead to a contradiction –
you can’t possibly satisfy all the assumptions at the same time.&lt;/p&gt;

&lt;p&gt;The bottom line is that trying to construct an interacting theory in
this fashion means it’s not &lt;em&gt;unitarily equivalent&lt;/em&gt; to the free theory.
Unitarily equivalence basically (skipping over a fair few details) means
that two theories can be described in the same mathematical context –
that they’re, in a vague sense, physically equivalent. Don’t take this
too seriously – the main point is unitarily equivalent = good,
unitarily inequivalent = bad. In non-relativistic quantum mechanics, it
turns out that any way to write down a proper quantum theory is
unitarily equivalent to all the other ways, even if it looks different
at first sight. In relativistic QFT, no such luck.&lt;/p&gt;

&lt;p&gt;So basically, Haag’s theorem says QFT doesn’t work. This is one of the
points Fraser makes in her article – how could we trust such a theory
for foundational work? Unless..&lt;/p&gt;

&lt;h1 id=&quot;underdetermination&quot;&gt;Underdetermination&lt;/h1&gt;

&lt;p&gt;Underdetermination of theory by evidence – &quot;underdetermination&quot; for
short – can basically (simplifying a bit again) mean two situations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two theories work just as well given all available evidence. Both
theories explain the evidence that has been gathered.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two theories work just as well given not only all available
evidence, but even all evidence that could possibly be gathered. Any
empirical predictions from the two theories will always agree.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first variant is very common; it frequently happens that there isn’t
enough evidence yet to favor one theory over another, but often later
evidence appears to rule one of the theories out.&lt;/p&gt;

&lt;p&gt;The latter case is more interesting and that is what Fraser has in mind
in her paper: the predictions of AQFT and CQFT actually do agree to any
given accuracy no matter what where AQFT exists.&lt;/p&gt;

&lt;p&gt;The problem in the context of the article is that AQFT doesn’t exist for
four spacetime dimensions, so we can’t really say there is any
underdetermination there. In two dimensions, AQFT and CQFT are
underdetermined, though, and one could reasonably argue that AQFT is a
preferable theory.&lt;/p&gt;

&lt;h1 id=&quot;renormalization-and-the-renormalization-group&quot;&gt;Renormalization and the Renormalization Group&lt;/h1&gt;

&lt;p&gt;When physicists first started computing physical quantities with QFT,
they ran in to an annoying issue: every result was infinite. Various
people then invented ad-hoc methods to get around this infinity – they
basically swept it under the rug by various tricks that were justified
only in a haphazard intuitive fashion. This trickery was called
renormalization, presumably because they feared calling it
&quot;bullshit&quot; would have hindered the acceptance of the theory.&lt;/p&gt;

&lt;p&gt;You can do this in a few ways, and it’s not obvious any of them really
offer satisfactory physical reasons for renormalization. The basic idea
is that you cut off certain properties to cure the infinities. A
straightforward way to do this is to suppose that space is not
continuous and smooth, but rather consists of blocks. These might be
very tiny blocks indeed – far beyond the ability of human eyes or even
particle accelerators to detect – but blocks nonetheless. This removes
the infinities, and if you’re clever you can make the space smooth again
while keeping the infinities away. This involves changing, for example,
the physical mass of the particles in the theory to counterbalance the
infinity. It turns out that doing so also escapes Haag’s theorem, since
the procedure violates the assumptions going in to it.&lt;/p&gt;

&lt;p&gt;Well, why not do that then? Because it doesn’t look like our space is in
fact made of tiny blocks! True, the blocks might be invisible to us
because we don’t have sophisticated enough equipment to see them, but
the fact remains that we have no evidence for that. For practical
reasons, physicists nevertheless use this trick, because even if it is
foundationally unsatisfactory, it leads to correct experimental results.&lt;/p&gt;

&lt;p&gt;There is another twist to this story, however. The procedure, it turns
out, is not quite as &quot;imaginative&quot; as it seems – and Wallace uses
this in his defence of the ordinary physicist, who goes on happily
renormalizing.&lt;/p&gt;

&lt;p&gt;In the 1970s, a brilliant fellow by the name of Ken Wilson was working
on critical phenomena – physics of critical points, like when water
turns to vapor or freezes to ice. Phase transitions!&lt;/p&gt;

&lt;p&gt;He was working on a particular problem, which had to do with a block of
pure metal with slight impurities of other materials sprinkled in. He
realized that critical points were caused by phenomena at several scales
working simultaneously. Typically, when investigating a matter
theoretically, you assume one scale is relevant – either a high-energy
or a low-energy situation; long length scales or short length scales,
and so on. Wilson realized that critical points were so hard to
understand because they involved multiple scales, and you couldn’t
ignore any of them.&lt;/p&gt;

&lt;p&gt;Wilson didn’t just think that, though. He developed a theory of how
quantities change in different scales. He realized that phase
transitions generally happen in points where calculations at all scales
happen to agree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/magnetization_renorm.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The basic way you do this type of calculation is as follows. You first
calculate at one scale. Then you calculate it in a more fine-grained
fashion – for example, if you had a 3x3 = 9 blocks of spacetime, you
increase that to 9x9 = 81. Then you see what this finer calculation
looks like in the coarse-grained case – you divide the 9x9 grid to 3x3
grids and calculate your quantity by averaging over those big blocks. So
at the end, you have two results for the very same quantity.&lt;/p&gt;

&lt;p&gt;In the image, I’ve done this calculation for the magnetization of a
famous physics model (the Ising model) for magnetization in different
temperatures. Notice where the two lines cross each other? That’s where
the critical point is! That’s where both scales agree, and the Ising
model loses its magnetization (that generally happens to magnets – if
you heat them too much, they become non-magnetic).&lt;/p&gt;

&lt;p&gt;So what does this have to do with renormalization? Well, Wilson showed
that &lt;em&gt;renormalization in QFT basically is this process!&lt;/em&gt; The way you
choose the counter-terms to remove the infinities basically involves
writing them as a function of the scale, and the way you can solve the
dependence of e.g. physical mass on the scale is using Wilson’s
renormalization group equations. Didn’t I tell you the guy was
brilliant?&lt;/p&gt;

&lt;p&gt;Wilson made sense of renormalization and discovered a deep connection
between particle physics and solid matter physics. Soon after, he
discovered a Nobel prize medal around his neck. Today, this method is
used in various fields of physics (well – people have developed much
more sophisticated algorithms, but the same basic principle).&lt;/p&gt;

&lt;p&gt;What this has to do with our discussion is a bit more subtle. Basically,
Wallace says that Wilson’s work on renormalization makes it completely
reasonable to expect that if there’s a further theory – something
deeper than QFT – then of course QFT is going to have oddities in it.
That’s just a matter of scaling, like in the renormalization group
equations. We don’t know what the theory beyond QFT is, but it is not
unreasonable to expect it shows up as infinities in our current theory.&lt;/p&gt;

&lt;p&gt;For example, imagine we wanted to model the surface of a metal. Instead
of dealing with individual atoms, of which there are too many to handle,
we might instead describe the surface as a field (this is a method
actually used by condensed matter physicists!). Now if we try to
calculate something at distances smaller than the distance between atoms
on the surface, &lt;em&gt;obviously&lt;/em&gt; we’re going to get some sort of nonsensical
result – we’re modeling a bunch of atoms as a field, and if we try to
go in between them, we get nonsense, because we’ve replaced the granular
atomic surface with a continuous one by force. The small distances are
meaningless in our model. They’re beyond the range of application.&lt;/p&gt;

&lt;p&gt;Similarly, QFT probably has a scale at which it no longer makes sense.
And that’s fine – Wilson’s renormalization group work tells us what to
do with the scales we can access, and that the scales we can’t access
show up in the theory in the renormalization group equations only
indirectly. There’s no reason to panic if an infinity crops up when we
try to go beyond it.&lt;/p&gt;

&lt;h1 id=&quot;bringing-it-together&quot;&gt;Bringing It Together&lt;/h1&gt;

&lt;p&gt;So, where does that leave us? AQFT is mathematically rigorous, but
doesn’t exist in 4 dimensions. It offers a different view of the world
from CQFT, which is mathematically sloppy but has the virtue of
existing.&lt;/p&gt;

&lt;p&gt;What is the difference? Well, for example, AQFT doesn’t really have
quanta, which are basically particles. It does have something
approximately like that, but it’s not a fundamental part of it like it
is in CQFT.&lt;/p&gt;

&lt;p&gt;Fraser also points out that if there’s really a fundamental theory
beyond QFT, then we can expect there to be some conceptual continuity
between that theory and QFT. That is, we should expect they rely on
similar concepts. So if we’re relying on ill-defined concepts of CQFT,
then might stray in our search for the elusive more fundamental theory
– we’re trying to base it on the wrong concepts!&lt;/p&gt;

&lt;p&gt;Wallace wants to say that QFT is approximately true and thus offers us
an approximate guide to foundational philosophical work. Fraser wants to
answer the question: if QFT were rigorously defined and exactly true,
what does it say about the world?&lt;/p&gt;

&lt;p&gt;I think Fraser’s problem is that her proposed theory doesn’t exist. This
is a massive problem – if I started hitting on ladies by bragging about
my imaginary Ferrari, I would quickly encounter a problem. Even if we
conceded that CQFT is hopelessly bad as a theory (which it clearly
isn’t, since it is at least empirically succesful!), that would give us
no reason whatsoever to trust what AQFT says about short-distance
physics. Until a realistic AQFT theory exists – one that is capable of
reproducing the Standard Model – it seems foolish to trust anything it
says about physics.&lt;/p&gt;

&lt;p&gt;In any case, hopefully you learned something about the phraseology of
this field of philosophy. The articles I based this on are Wallace’s
&quot;Taking particle physics seriously: A critique of the algebraic
approach to quantum field theory&quot; (2011) and Fraser’s &quot;Quantum Field
Theory: Underdetermination, Inconsistency, and Idealization&quot; (2009).&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://quantumsoapco.com/&quot;&gt;https://quantumsoapco.com/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="quantum field theory" /><category term="philosophy" /><summary type="html">If you’re even remotely interested in the philosophy of physics, you’ve surely heard of quantum mechanics. Hell, nowadays quantum mechanics is even in your washing machine – who among us would use non-quantum soap?1 No doubt you’ve heard all sorts of wonderful things about quantum particles instantly knowing what happens to their entangled partner, or being in two places at once, and so on. https://quantumsoapco.com/ &amp;#8617;</summary></entry><entry><title type="html">Guessing Physical Laws</title><link href="http://localhost:4000/simplyphysics/general/2023/09/29/guessing-physical-laws.html" rel="alternate" type="text/html" title="Guessing Physical Laws" /><published>2023-09-29T00:00:00+03:00</published><updated>2023-09-29T00:00:00+03:00</updated><id>http://localhost:4000/simplyphysics/general/2023/09/29/guessing-physical-laws</id><content type="html" xml:base="http://localhost:4000/simplyphysics/general/2023/09/29/guessing-physical-laws.html">&lt;p&gt;Theoretical physics is a precise business. You choose your principles
carefully, put them in to mathematical form and use rigorous logical
deduction to infer consequences. Or perhaps you take a hithertho
unexplained phenomenon and start from a known physical theory, using
judicious approximations and clever calculation techniques to produce a
brilliant explanation for its features. You might even write a computer
program to apply numerical methods to theories and get your results that
way. Either way, it’s clearly a job for the logical goober! That’s how
you should do physics.&lt;/p&gt;

&lt;p&gt;Well, except if you just want to play a guessing game. I mean…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bohr deduced his famous atomic model by guessing physical principles
that were totally wrong, he managed to get the right result anyway;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maxwell used gears and other mechanical devices to explain his
theory of electromagnetism, and those turned out to be totally
useless;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Modified Newtonian dynamics was originally based on just guessing a
generic form for Newton’s law of gravity;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dirac derived his equation using mathematical methods and simply
guessed the positron solutions were actually real particles&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and on and on it goes.&lt;/p&gt;

&lt;p&gt;So, hell, it’s pretty useful to know how to just wing it.
Back-of-the-envelope calculations and leaps of logic abound in physics
departments everywhere, and great discoveries are often made just as
much by inspired guesswork as they are by logic. Still, there’s rules
even to leaps; no matter how magical Michael Jordan’s jumps look, he
can’t actually fly. Well, I don’t think so anyway. To be fair, a few of
the videos I’ve seen seem to defy the laws of physics – for the sake of
argument, suppose he doesn’t know how to fly to make my point make
sense. Magical guesswork isn’t actually magical!&lt;/p&gt;

&lt;p&gt;In this text, I’ll go over a few methods to just guess, using simple
physical intuition, one particular example of a physical law: the
Maxwell-Boltzmann speed distribution.&lt;/p&gt;

&lt;h1 id=&quot;what-the-hell-do-you-know-anyway&quot;&gt;What the Hell Do You Know, Anyway?&lt;/h1&gt;

&lt;p&gt;Our problem is to find the distribution of the speed of gas molecules in
a room full of molecules. What is it that we know about this problem?&lt;/p&gt;

&lt;p&gt;Well, any schoolboy knows that rooms feel different at different
temperatures. Even a 5 year old pipsqueak can tell you that temperature
differences are caused by the speed of the molecules whizzing about in
the room. You can guess that the kind of the molecules matters, too –
different speeds for different types. And you probably already knew in
your mother’s womb that distributions are normalized; that is, if we
have a speed distribution, then&lt;/p&gt;

\[\int f(v) dv = 1\]

&lt;p&gt;with $f(v)$ our mysterious distribution. We can perhaps make a further
guess: presumably there’s nothing special about any of the directions –
the distribution looks the same in $x$, $y$ and $z$ directions, so we
can deal with them separately.&lt;/p&gt;

&lt;p&gt;Well, you say, without further information this doesn’t amount to much.
Unless I can use the principles I learned in my statistical physics
class, there’s no way to make progress!&lt;/p&gt;

&lt;p&gt;Or that’s what you would say if you were a &lt;em&gt;pathetic little weakling.&lt;/em&gt;
Are you a pathetic weakling? No? Then let me introduce you to
dimensional analysis.&lt;/p&gt;

&lt;h1 id=&quot;theres-rules-to-combining-quantities&quot;&gt;There’s Rules To Combining Quantities&lt;/h1&gt;

&lt;p&gt;You see, we just identified a number of things a speed distribution
probably depends on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The actual speed of the particles (obviously, since it’s a speed
distribution)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The kind of molecule&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The temperature of the room&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first one is evident, but the other two aren’t. Let’s talk about
them.&lt;/p&gt;

&lt;p&gt;First of all, we’re not really interested in the kind of molecule in the
room, just some property of it. Unless you’re feeling confident and
think you can solve this problem by starting from the quantum mechanics
of molecules (if so, good luck, see you in 5 years). No, what we really
want is just the inertia of the molecule, how hard it is to accelerate.
In principle, if there were enough molecules in the room, or if they
were big enough, then we might have to care about intermolecular
interactions, but let’s just suppose they’re far enough apart to not
really interact in a meaningful way. So we want the &lt;em&gt;mass&lt;/em&gt; of the
molecules.&lt;/p&gt;

&lt;p&gt;As for the temperature, I already told you that even a schoolboy knows
temperature differences are measured by the speed – or more accurately,
the kinetic energies – of the particles. So let’s simply call the
quantity of interest here &quot;energy&quot;, for that is in fact what it is.&lt;/p&gt;

&lt;p&gt;What now? We have three quantities, $v, m$ and $E$. We also know the
normalization of the distribution. But this normalization gives us our
first clue.&lt;/p&gt;

&lt;p&gt;Notice how the end result of $\int f(v) dv$ is a pure number – no
dimension? Let’s call the units of mass $[m] = M$, units of time $T$ and
units of length $L$ (these could be anything – meters, inches, a fruit
fly’s average wingspans – as long as you keep your units consistent).
Then we know that $[f(v)] = [1/v] = T/L$, because $[dv]=L/T$. See how
that works? The units of $f$ have to cancel out the units of integration
measure $dv$, since the end result 1 doesn’t have units.&lt;/p&gt;

&lt;p&gt;So we’re looking for a distribution that has units of $T/L$. How could
we combine our quantities $m$, $E$ and $v$ to get units like that?
Notice that $[E] = ML^2/T^2$ Think about it for a while, come up with
options. Here are the easiest options I came up with:&lt;/p&gt;

\[\begin{align}f(v) &amp;amp;\propto 1/v g\bigg(\frac{v^2m}{E}\bigg)\\
    f(v) &amp;amp;\propto \sqrt{\frac{m}{E}}g\bigg(\frac{v^2m}{E}\bigg) \\
    f(v) &amp;amp;\propto \frac{vm}{E}g\bigg(\frac{v^2m}{E}\bigg)\end{align}\]

&lt;p&gt;The function $g$ is unknown, but must be a function of dimensionless
quantity, since $1/v$ and $\sqrt{\frac{m}{E}}$ and $\frac{vm}{E}$
already have the right dimensions (check for yourself!). The simplest
dimensionless quantity you can construct from $v$, $m$ and $E$ is
$v^2m/E$.&lt;/p&gt;

&lt;p&gt;Alright, is there any way to eliminate any of these possibilities using
our intuition? How do you think the distribution of the speeds should
behave?&lt;/p&gt;

&lt;p&gt;Well, I don’t know about you, but I don’t think that the distribution
blowing up as $v\rightarrow 0$ seems very reasonable. I mean, maybe the
slower speeds are more likely, but is that prefactor really supposed to
be divergent? How would you even find a reasonable $g$ that would still
integrate to a finite number over the infinite interval
$v\in [0, \infty)$?&lt;/p&gt;

&lt;p&gt;As for the third one, it suggests that the distribution is proportional
directly to the speed and mass of the particles. Does it seem likely
that $f(0) = 0$? Really, just a straight up zero at the origin? Maybe
it’s possible – we can keep it in mind.&lt;/p&gt;

&lt;p&gt;However, the middle one is the least offensive choice for at least my
sensibilities (I also happen to know this guess produces the right
answer, so my &quot;intuition&quot; is greatly aided by foreknowledge!) Since
the dimensionful part doesn’t contain $v$, there’s no immediate
pathologies that jump out.&lt;/p&gt;

&lt;p&gt;Let’s go with the middle guess there. Our distribution is of the form
(with $\alpha$ and $\gamma$) some yet to be determined constants)&lt;/p&gt;

\[f(v) = \alpha \sqrt{\frac{m}{E}} g\bigg(\gamma \frac{v^2m}{E}\bigg)\]

&lt;p&gt;We now need to find an appropriate $g$. It has to be some function that
is integrable from 0 to $\infty$. Presumably, the speeds closer to 0 are
way more likely than the tail end – we don’t have too many infinitely
fast particles flying about, that would just be straight up painful. Try
to find some functions that satisfy that property. Go on, I’ll wait.&lt;/p&gt;

&lt;p&gt;You can probably find a bunch. You can plot them to see what they would
look like, just pick some easy values for $m$ and $E$. However, we now
remember the Central Limit Theorem, which says that everything is always
the damn exponential distribution (well, it doesn’t quite say that, but
close enough), so that&lt;/p&gt;

\[g\bigg(\frac{v^2m}{E}\bigg) = e^{-\frac{\gamma v^2m}{E}}\]

&lt;p&gt;Plot this – it certainly seems reasonable, right?&lt;/p&gt;

&lt;p&gt;Supposing we’re right, is there some way to go even further? Can we
guess $\alpha$ and $\gamma$?&lt;/p&gt;

&lt;p&gt;Well, notice what we have in the exponential function: $mv^2$. Does that
look like any form of energy you know of?&lt;/p&gt;

&lt;p&gt;Right, it’s the kinetic energy of a particle if we take
$\gamma = \frac{1}{2}$. So let’s plug that in! And, lucky for us, we now
remember the normalization of the distribution:&lt;/p&gt;

\[\int _0^\infty f(v)dv =\int _0^\infty\alpha \sqrt{\frac{m}{E}} g\bigg( \frac{v^2m}{2E}\bigg)dv = 1 \\
    \implies \alpha \frac{1}{2}\sqrt{2\pi } = 1 \\
    \implies \alpha = \sqrt{\frac{2}{\pi }}\]

&lt;p&gt;So, our answer is&lt;/p&gt;

\[f(v) = \sqrt{\frac{2}{\pi }} \sqrt{\frac{m}{E}}e^{-\frac{\gamma v^2m}{E}}\]

&lt;p&gt;which, wouldn’t you know it, is exactly right.&lt;/p&gt;

&lt;h1 id=&quot;you-cheated&quot;&gt;You Cheated!&lt;/h1&gt;

&lt;p&gt;&quot;Ah yes&quot;, you say, &quot;of course you’re able to do this because you
cheated! You already knew the end result, and used that as a guide! You
can’t use this for anything real!&quot;&lt;/p&gt;

&lt;p&gt;First of all, who the hell do you think you are, assaulting me with the
truth??&lt;/p&gt;

&lt;p&gt;It’s not quite that simple, though. As I said, real physicists making
real discoveries have used various forms of guesswork all throughout
history (see Anthony Zee’s book &quot;Fly by Night Physics&quot; for plenty of
examples of exactly this sort of historical reasoning). Order of
magnitude estimates, dimensional analysis, intuitively inspired
approximations – all of these are tricks you can find in the literature
that have really been used for research, not just when you know the end
result.&lt;/p&gt;

&lt;p&gt;It might also help you if you’re a student. Suppose you only very
vaguely remember some formula or end result of a calculation, and it’s
asked on the exam. Well, if you mostly remember it – like was the case
for me and the Maxwell-Boltzmann distribution when I did this
calculation – you can guess your way to the right solution using this
kind of reasoning. Mr. Zero Points turns in to Mr. Partial Credit.&lt;/p&gt;

&lt;p&gt;What are the principles we used here?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Identify the relevant quantities at play; in our case, mass, energy,
speed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make the necessary approximations. We implicitly approximated that
the particles are non-relativistic, and we don’t care about
intermolecular forces or quantum mechanical effects; basically, it’s
a normal room full of something like air.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform dimensional analysis on the quantity you’re interested in
calculating. Find any combinations of the quantities that you
identified that give the right units and satisfy any other
constraints you identified. Eliminate the combinations that don’t
fit the constraints; if more than one combination remains, pick the
one that seems right to you based on your intuition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Try to fix any undetermined constants by some bullshit argument if
possible; if not, just leave them there.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is clear that this sort of reasoning can’t give you the same
certainty as a logical deduction from well-justified principles. It’s
equally clear that it’s far easier to use this method than to come up
with the correct fundamental principles for a problem you’re not all
that familiar with. If you got it wrong – eh, it happens, whatever.&lt;/p&gt;</content><author><name></name></author><category term="general" /><category term="statistical physics" /><summary type="html">Theoretical physics is a precise business. You choose your principles carefully, put them in to mathematical form and use rigorous logical deduction to infer consequences. Or perhaps you take a hithertho unexplained phenomenon and start from a known physical theory, using judicious approximations and clever calculation techniques to produce a brilliant explanation for its features. You might even write a computer program to apply numerical methods to theories and get your results that way. Either way, it’s clearly a job for the logical goober! That’s how you should do physics.</summary></entry><entry><title type="html">An Emergency Help Document for Those Horrified, Disgusted or Otherwise Afflicted by Infinities in QFT</title><link href="http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/09/15/horrifying-infinities.html" rel="alternate" type="text/html" title="An Emergency Help Document for Those Horrified, Disgusted or Otherwise Afflicted by Infinities in QFT" /><published>2023-09-15T00:00:00+03:00</published><updated>2023-09-15T00:00:00+03:00</updated><id>http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/09/15/horrifying-infinities</id><content type="html" xml:base="http://localhost:4000/simplyphysics/quantum%20field%20theory/2023/09/15/horrifying-infinities.html">&lt;p&gt;You’re listening to a lecture on quantum field theory. Finally, the
equations of motion have been derived, the fields quantized and the
reduction formulae derived. It is finally time to solve a real QFT
problem – to do a perturbation theory calculation!&lt;/p&gt;

&lt;p&gt;All of a sudden – unprompted and unwanted – the calculation leads to a
divergent integral. WHAT?? How can a physical quantity be infinite?&lt;/p&gt;

&lt;p&gt;Before you descend in to a frenzy, the lecturer provides much-needed
assurance. There’s no problem, he explains – all we have to do is to
regularize and renormalize! Just pretend the integral is finite after
all by imposing some cutoff $\Lambda$ (&quot;regularize&quot;), and add some
terms to your theory that remove the terms that go to infinity as the
cutoff $\Lambda$ is removed (&quot;renormalize&quot;).&lt;/p&gt;

&lt;p&gt;You’re horrified, of course. What do you mean, $renormalize?$ How can
you just go ahead and pretend the integral is finite, then erase the
terms you didn’t like? Curse this quantum field theory; it’s all
nonsense anyway; there’s no way this can be rigorous; I should’ve
studied art history..&lt;/p&gt;

&lt;p&gt;I’m here to tell you that the situation with infinities isn’t as bad as
all that – it’s much worse. I’m also here to tell you that this is no
reason to switch to art history. What sort of a coward fears a few
measly infinities?&lt;/p&gt;

&lt;h1 id=&quot;infinities-everywhere&quot;&gt;Infinities everywhere&lt;/h1&gt;

&lt;p&gt;Instead of dealing with a difficult theory like QFT, let’s deal with
something our tiny brains are more in tune with: polynomials and their
roots.&lt;/p&gt;

&lt;p&gt;That’s right, it’s back to school. Polynomials! You can do them using
perturbation theory, too.&lt;/p&gt;

&lt;p&gt;Suppose our polynomial is&lt;/p&gt;

\[\begin{align}
    \epsilon x^2 - 2x + 1 = 0 \label{eq:seconddegree}\end{align}\]

&lt;p&gt;and we say $\epsilon$ is a small number. Then we might guess the answer
is of the form&lt;/p&gt;

\[\begin{equation}
    x = x_0 + \epsilon x_1 + \mathcal{O}(\epsilon ^2)\end{equation}\]

&lt;p&gt;Let’s insert our guess in to our polynomial:&lt;/p&gt;

\[\begin{equation}
    \epsilon x^2 - 2x + 1 = \epsilon x_0^2 - 2(x_0 + \epsilon x_1)+1 = 0.\end{equation}\]

&lt;p&gt;We’re neglecting all but first order terms. Remember that this must hold
for arbitrary small $\epsilon$, so that we can divide this in to two
equations, one involving the terms of zeroth order in $\epsilon$, and a
first order equation. Like so:&lt;/p&gt;

\[\begin{equation}
    - 2x_0 + 1 + \epsilon (x_0^2 - 2x_1)  = 0\\
    \implies \begin{cases}
        - 2x_0 + 1   = 0\\
        x_0^2 - 2x_1  = 0
    \end{cases}\end{equation}\]

&lt;p&gt;Well, sure enough, we can solve the first equation easily: the solution
is $x_0 = \frac{1}{2}$. Inserting that in to the second equation, we get&lt;/p&gt;

\[\begin{equation}
    x_0^2 - 2x_1 = 0 \implies x_1 = \frac{1}{8}.\end{equation}\]

&lt;p&gt;So the solution to our perturbation problem is
$x = \frac{1}{2} + \frac{1}{8}\epsilon$. Easy!&lt;/p&gt;

&lt;p&gt;But wait – what? Second degree polynomials have two roots, everyone
knows that. What happened?&lt;/p&gt;

&lt;p&gt;Well, we’ve all learned the formula for solving second degree
polynomials, so let’s apply it. The exact solution of
$\eqref{eq:seconddegree}$ is&lt;/p&gt;

\[\begin{equation}
    x_-  = \frac{1-\sqrt{1-\epsilon}}{\epsilon}\\
    x_+  = \frac{1+\sqrt{1-\epsilon}}{\epsilon}.\end{equation}\]

&lt;p&gt;Well, why not apply perturbation theory to this form? Expanding around
$\epsilon = 0$ using a Taylor series, we immediately get&lt;/p&gt;

\[\begin{equation}
    x_-  = \frac{1}{2} + \frac{1}{8}\epsilon + \mathcal{O}(\epsilon ^2)\\
    x_+  = \frac{2}{\epsilon } - \frac{1}{2} + O(\epsilon ).\end{equation}\]

&lt;p&gt;Oh boy – as we set $\epsilon \rightarrow 0$, the second term solution
goes towards infinity. Curse these second degree polynomials; they’re
nonsense anyway; there’s no way this is rigorous; I should’ve studied
art history..&lt;/p&gt;

&lt;p&gt;The reader might be feeling quite betrayed. Have we really been beaten
by a second degree polynomial of all things? Can’t a fella even trust
elementary school algebra these days? Surely, when we &lt;em&gt;know&lt;/em&gt; the answer
ahead of time, we must somehow be able to get a sense of it
perturbatively!&lt;/p&gt;

&lt;p&gt;We can. You see, the choice of equation
$\eqref{eq:seconddegree}$ is not really unique, is it? We could
multiply it on both sides by some number, and it would still be zero. We
could then do the reverse operation at the end to get the roots in terms
of our original variable $x$.&lt;/p&gt;

&lt;p&gt;This helps us, because our problem clearly arises from the fact that
we’re treating the highest order part of the polynomial perturbatively.
Since removing that term reduces the number of roots by one, we can’t
possibly find the second root by applying perturbation theory naively.
We have to get rid of the $\epsilon$ in front of $x^2$ and move it to
some other term by scaling $x$. In other words, we have to renormalize.
Indeed, define a new variable $w = \epsilon x$. Then
$\eqref{eq:seconddegree}$ is&lt;/p&gt;

\[\begin{equation}
    \epsilon x^2 - 2x + 1 \rightarrow \frac{1}{\epsilon ^2}w^2 - 2 \frac{w}{\epsilon} +1 = 0 \iff w^2 - 2\epsilon w + \epsilon ^2 = 0\end{equation}\]

&lt;p&gt;Behold – all of our problems have disappeared. This form amenable to a
perturbation treatment. You can solve this using the perturbation
procedure outlined above and get a perfectly finite answer, and all we
did is rescale the problem, just as you do in renormalizing QFT.&lt;/p&gt;

&lt;p&gt;In some sense, the original variable $x$ just wasn’t good for what we
were looking to do. We couldn’t recover two roots for a second order
polynomial using that variable.&lt;/p&gt;

&lt;p&gt;Of course, this problem is quite general with polynomial equations.
Without changing variables, you’ll always get fewer roots than you
should, and in fact the roots invisible to you are going to be sensitive
to the perturbation parameter. Look at this polynomial:&lt;/p&gt;

\[\begin{equation}
    \epsilon x^6 + x^2 - 2x + 1 = 0 \label{eq:sixth}\end{equation}\]

&lt;p&gt;The absolute values of as a function of $\epsilon$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/zero_sizes.png&quot; alt=&quot;The zeros.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how some of the values are highly sensitive to $\epsilon$, and
they escape towards infinity as epsilon gets smaller (you can only see
three, because they’re complex and have similar absolute values). Those
are the values you wouldn’t find with perturbation theory.&lt;/p&gt;

&lt;p&gt;Alright, you say, maybe this renormalization business isn’t so bad after
all. I can now sleep my nights well and live without the constant
anxiety caused by lurking infinities.&lt;/p&gt;

&lt;p&gt;Hah, we’ve barely gotten started. If you thought our problems with
quantum field theory end with the singularities of &lt;em&gt;polynomials&lt;/em&gt;, then I
have bad news for you. Here’s another serving of despair.&lt;/p&gt;

&lt;h1 id=&quot;i-got-99-problems&quot;&gt;I got 99 problems&lt;/h1&gt;

&lt;p&gt;You see, just because you have managed to produce some finite terms in
your little perturbation series, that doesn’t mean that the sum of those
finite terms is in fact finite. Hell, look at this:&lt;/p&gt;

\[\begin{equation}
    S = \sum _{n=0}^\infty 2^n\end{equation}\]

&lt;p&gt;Doesn’t look very finite, does it? So how do you know your series gives
finite results?&lt;/p&gt;

&lt;p&gt;To illustrate some divergent series, let’s take a nice normal function,
not this crazy path integral business. Behold our guinea pig:&lt;/p&gt;

\[\begin{equation}
    Z = \int dxe^{-x^2 - \lambda x^4} \label{eq:normal}\end{equation}\]

&lt;p&gt;Those familiar with QFT will note the suggestive name and form of this
integral! Now evidently, if we have no &quot;interaction&quot; (the $x^4$ term),
we get the result:&lt;/p&gt;

\[\begin{equation}
    Z = \sqrt{\pi}\end{equation}\]

&lt;p&gt;The full solution to $\eqref{eq:normal}$ is&lt;/p&gt;

\[\begin{equation}
    \int e^{-x^2 - \lambda x^4}dx = \frac{e^{\frac{1}{8\lambda}}K_{1/4}\bigg(\frac{1}{8\lambda}\bigg)}{2\lambda} \label{eq:finite}\end{equation}\]

&lt;p&gt;where $K$ is the modified Bessel function of the second kind – though
it is not really important for our purposes here. The point is, in this
case, we can solve the problem even without a &quot;perturbation series&quot;.
But anyway, let us proceed as we would in field theory, where we want to
treat the $\phi ^4$ term perturbatively. First&lt;/p&gt;

\[\begin{equation}
    \int d^4x e^{-x^2 - \lambda x^4} = \int d^4x e^{-x^2}e^{-\lambda x^4} = \sum _{n=0}^\infty \int d^4x e^{-x^2} \frac{(-\lambda x^4)^n}{n!}\end{equation}\]

&lt;p&gt;Easy peasy. After some integrations and algebra, we get&lt;/p&gt;

\[\begin{equation}
    Z = \sum _{n=0}^\infty \sqrt{\pi}\frac{(-\lambda)^n(4n)!}{2^{4n}(2n)!n!} \label{eq:seriesexp}\end{equation}\]

&lt;p&gt;Check with your favorite tool – this series is divergent.&lt;/p&gt;

&lt;p&gt;Hold on! We started with a perfectly finite integral $\eqref{eq:finite}$, and ended up with an infinite series? Mumble mumble art history..&lt;/p&gt;

&lt;p&gt;Here’s a question for you: when are you allowed to exchange the order of
series and integral? Hint: not in this case. So we’ve created our
problems by illegally changing the order of those two operations.
Frankly, I’m shocked that a SWAT team has not bursted through my door as
I write this – that’s how bad our math is. With finite intervals and
finite sums, interchanging them is pretty much always allowed; but in
the case of series and improper integrals, we have to be more careful
than this.&lt;/p&gt;

&lt;p&gt;The series in $\eqref{eq:seriesexp}$ is not hopeless, though. Here’s a funny plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/inf.png&quot; alt=&quot;Value of the series as a function of the number of terms
included.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The red line is the correct answer calculated with the Bessel function.
So you see, as long as you cut off the series before it gets wild, you
will get a useful approximation. Essentially, we will get punished for
our lack of mathematical rigor after some number of terms in the series,
but not immediately; the first few terms give increasingly accurate
approximations for our integral.&lt;/p&gt;

&lt;p&gt;In fact, we can give an estimation as to when the series will start to
diverge, but I won’t do so here for a simple reason: the estimate for
quantum electrodynamics is about $\mathcal{O}(100)$. That means we could
calculate dozens of terms in the series before running in to any trouble
with divergences. However, only the first 5 or 6 orders have been
calculated, and it’s unlikely we’ll ever get much further than that,
since the number of Feynman diagrams in each term of the series keeps
increasing very rapidly. It’s also not particularly useful to calculate
more terms given the limits of current experiments – why calculate more
terms than experimental accuracy allows for?&lt;/p&gt;

&lt;p&gt;The point with regard to QFT is this: in forming a perturbation series
in the (path integral) quantum field theory, we actually do a similar
swap between integral and sum (see next section). That is why the series
in QFT is divergent.&lt;/p&gt;

&lt;p&gt;I have now battered you with two types of infinities – infinities in
individual expressions and infinities in series summation – and solved
them easily (hey, just like, cut off your series man!). What does this
have to do with QFT exactly? Well, I’ve already given you some hints.
The following section contains some detail that you can skip if you
don’t care.&lt;/p&gt;

&lt;h1 id=&quot;what-about-qft&quot;&gt;What About QFT?&lt;/h1&gt;

&lt;p&gt;The main point of this text is that the infinities in QFT are not
insurmountable, and that the problems discussed previously are in some
sense analogous to problems in QFT. If you don’t care about QFT details,
then you can skip this section.&lt;/p&gt;

&lt;p&gt;Here’s how you do quantum field theory in the path integral formalism
(which we use because it most conveniently illustrates the problem,
though it also applies to the other formalisms). To get the propagator,
you sum over all the fields like so:&lt;/p&gt;

\[\begin{equation}
    \langle 0 | T \phi (x) \phi (x&apos;) |0\rangle = \int D\phi \phi (x) \phi (x&apos;)e^{-\int d^4x \mathcal{L}} \label{eq:measure}\\
    \mathcal{L} = \frac{1}{2}\phi (x)(\partial ^\mu \partial _\mu - m^2)\phi(x) - \lambda \phi ^4 + J(x)\phi (x)\end{equation}\]

&lt;p&gt;Intuitively, this means &quot;integrate over all possible field
configurations&quot;. So if you were to divide your space in to little
squares, forming a grid, then this integral would be&lt;/p&gt;

\[\begin{equation}
    \int D \phi F[\phi] = \int d\phi _1 d\phi _2 ..d\phi _n F(\phi _1, \phi _2 ... \phi _n) \label{eq:fi}\end{equation}\]

&lt;p&gt;with $\phi _i$ the value of the field at grid point $i$. In an intuitive
sense, the integral in $\eqref{eq:measure}$ is the limit of $\eqref{eq:fi}$ as
$n\rightarrow \infty$.&lt;/p&gt;

&lt;p&gt;Let us also define&lt;/p&gt;

\[\begin{equation}
    Z[\phi] = \int D\phi e^{-\int d^4 x[ \mathcal{L} + J(x)\phi (x)]} \label{eq:Z}\end{equation}\]

&lt;p&gt;Here, $\phi ^4$ is our interaction term and $J$ is an arbitrary source,
such as some sort of magnetic field we wish to treat classically. Now it
is easy to see that we can split this thing to two parts&lt;/p&gt;

\[\begin{equation}
    \int D\phi \phi (x) \phi (x&apos;) e^{-\int d^4x \mathcal{L}} \\
     = \int D\phi \phi (x) \phi (x&apos;)e^{-\int d^4x \frac{1}{2}\phi (x)(\partial ^\mu \partial _\mu - m^2)\phi(x)+J(x)\phi (x)}e^{-\int d^4x \lambda \phi ^4} \label{eq:propagator}\end{equation}\]

&lt;p&gt;Now, it is more obvious than obvious that we can write the last
exponential term, which we have conveniently separated from the rest, as
a series&lt;/p&gt;

\[\begin{equation}
    e^{-\lambda x^4}=\sum _{n=0}^\infty \frac{(-\lambda ^nx^{4n})}{n!}\end{equation}\]

&lt;p&gt;By some tricks that I won’t go in to here, you can write&lt;/p&gt;

\[\begin{equation}
    \int D\phi e^{-\int d^4x \frac{1}{2}\phi (x)(\partial ^\mu \partial _\mu - m^2)\phi(x)+J(x)\phi (x)} = C e^{-\int d^4x J(x)G_F(x-y)J(y)}\end{equation}\]

&lt;p&gt;Here $G_F$ is the Green’s function, just as you know from QFT (this
result is valid even when there are prefactors in front of the
exponential – sort of). $C$ is a constant that won’t concern us here.&lt;/p&gt;

&lt;p&gt;We now note that, since&lt;/p&gt;

\[\begin{equation}
    e^{-\lambda x^4}=\sum _{n=0}^\infty \frac{(-\lambda x^{4})^n}{n!}\end{equation}\]

&lt;p&gt;we can write this in terms of derivatives of $J$. You see,
$\frac{\delta}{\delta J}e^{J\phi} = \phi e^{J\phi}$, so that&lt;/p&gt;

\[\begin{equation}
    \sum _{n=0}^\infty \frac{(-\lambda ^nx^{4n})}{n!} = \sum _n e^{\bigg( \frac{\delta}{\delta J(z)} \bigg)^{4n}}e^{Jx }\end{equation}\]

&lt;p&gt;See how that works? Of course, the two $\phi$ - fields in front of the
exponential in $\eqref{eq:propagator}$ can also be written as
$\frac{\delta}{\delta J(x)}\frac{\delta}{\delta J(x’)}Z[\phi]$ (recall
the definition of $Z$ from $\eqref{eq:Z}$). So putting this all together, the propagator is:&lt;/p&gt;

\[\begin{equation}
    \langle 0 |T \phi (x) \phi (x&apos;)|0\rangle = C\frac{\delta }{\delta J(x)}\frac{\delta }{\delta J(x&apos;)}\sum _{n=0}^\infty \frac{\lambda ^n \frac{\delta ^{4n}}{\delta J(z)^{4n}}}{n!}Z[\phi ]\end{equation}\]

&lt;p&gt;Right? All we did is write a few terms with derivatives of $J$ and
expand the interaction term $\lambda \phi ^4$ as a series. Easy! Only
thing is.. this series diverges, even if you manage to renormalize.&lt;/p&gt;

&lt;p&gt;What? Curse this quantum field.. well, by now, you catch my drift.&lt;/p&gt;

&lt;p&gt;You see, we’ve made a bit of a blunder here. We’ve exchanged the sum
from $n=0$ to $\infty$ with the integral in $Z[\phi]$. And we can’t do
that in this instance – as we just saw with the exponential function!&lt;/p&gt;

&lt;p&gt;Well then, you say, why even bother swapping the integral and the
series? Just do the integral once and for all!&lt;/p&gt;

&lt;p&gt;If you can do that, go ahead. I dare you. And when you’ve solved the
full interacting theory like that, send me an email with the solution –
I promise I totally won’t publish it without mentioning you and steal
the Nobel.&lt;/p&gt;

&lt;p&gt;The story with renormalization of individual terms of a series is the
same. We use an inappropriate variable (&quot;bare mass&quot;, &quot;bare coupling
constant&quot;, etc) to describe our theory and we get punished for it.&lt;/p&gt;

&lt;h1 id=&quot;the-problems-never-end&quot;&gt;The Problems Never End&lt;/h1&gt;

&lt;p&gt;So, hopefully you’re convinced that infinities in themselves are no
cause for panic. Still, quantum field theory is not without its
problems. There are other mathematical issues in the formalism which
have not been resolved. The best attempt at a rigorous QFT is the
algebraic QFT framework – but nobody has even managed to formulate
4-dimensional interacting theories in it; renormalization in QFT is
essentially an unsolved problem, if you want total mathematical rigor.&lt;/p&gt;

&lt;p&gt;Nevertheless, I hope this has helped to alleviate some of your disgust
about the infinities in QFT. The divergences, though technically
formidable compared to polynomials and simple integrals of one variable,
are not really that concerning. Just as in the case of polynomials and
exponential functions, there are reasonable ways to deal with the
infinities that don’t involve &quot;sweeping them under the rug&quot;. This fact
is simply obscured by the considerable technical difficulty in doing
field theory calculations.&lt;/p&gt;

&lt;p&gt;For practical purposes, QFT is just fine.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The series material is lifted from &quot;How I Learned To Stop Worrying and
Love QFT&quot; by R. C. Helling. The first polynomial perturbation example
is standard, found for example in &quot;A First Look at Perturbation
Theory&quot; by Simmonds and Mann. I can’t take credit for most of the
ideas; I combined aspects of both of these sources with a bit of my own
stuff added in to hopefully assist students struggling in QFT courses.&lt;/p&gt;</content><author><name></name></author><category term="quantum field theory" /><category term="quantum field theory" /><summary type="html">You’re listening to a lecture on quantum field theory. Finally, the equations of motion have been derived, the fields quantized and the reduction formulae derived. It is finally time to solve a real QFT problem – to do a perturbation theory calculation!</summary></entry><entry><title type="html">A Gentle Introduction to Computational Quantum Mechanics</title><link href="http://localhost:4000/simplyphysics/computational%20quantum%20mechanics/2023/08/29/intro-to-computational-qm.html" rel="alternate" type="text/html" title="A Gentle Introduction to Computational Quantum Mechanics" /><published>2023-08-29T00:00:00+03:00</published><updated>2023-08-29T00:00:00+03:00</updated><id>http://localhost:4000/simplyphysics/computational%20quantum%20mechanics/2023/08/29/intro-to-computational-qm</id><content type="html" xml:base="http://localhost:4000/simplyphysics/computational%20quantum%20mechanics/2023/08/29/intro-to-computational-qm.html">&lt;p&gt;This little text is intended as a very simple introduction to
computational quantum mechanics. There are many books devoted to the
topic, filled with one complicated algorithm after the other. Many
people devote their whole careers to computational quantum mechanics.
It’s harder to find something that allows a total beginner to get
started.&lt;/p&gt;

&lt;p&gt;Unfortunate, isn’t it? After all, you don’t teach a fella how to drive
by putting him in a Formula 1 car; you don’t teach a soldier to fire a
weapon by placing him in front line combat; you can’t teach a child to
run a marathon when she has yet to take her first steps.&lt;/p&gt;

&lt;p&gt;Fortunately, it’s not too difficult to write programs that solve the
quantum equations on a computer. In fact, we can solve the problem and
plot the solution in just a few lines of Python code. To understand
those lines, though, takes a little bit of work. It’s easy to see a
painting and say &quot;well, just take a brush and move it around the paper
a bit!&quot; It’s a bit harder to do that in practice. In the cases we’ll
solve, though, it’s not that difficult. All I require of the reader is
the basics of linear algebra, calculus and some programming knowledge,
that’s all; the linear algebra and calculus are a bare minimum for
quantum mechanics, and programming should go without saying since we’re
doing computational work!&lt;/p&gt;

&lt;p&gt;Hopefully the reader doesn’t already feel betrayed — I did say I
wouldn’t teach a kid to run a marathon before they can walk. In this
case, though, knowing some calculus and linear algebra is the equivalent
of standing up. If you haven’t yet heard of those, you have two options:
don’t read further or just blindly copy the code to see the end results
to get yourself motivated to learn calculus.&lt;/p&gt;

&lt;p&gt;I will program in Python, so to follow along, make sure you have numpy
and matplotlib installed. Then write these at the top of your file:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We’re ready to go! We’ll solve the time-independent Schrödinger equation
in one dimension&lt;/p&gt;

\[\begin{aligned}
    \bigg( -\frac{1}{2} \frac{d^2}{dx^2} + V(x)\bigg)\psi (x) = E\psi (x)\end{aligned}\]

&lt;p&gt;If you’ve taken quantum mechanics, you might have heard that this is an
eigenvalue equation, and that’s one of the reasons Schrödinger’s
equation produces &quot;quantization&quot; (discrete energy levels with allowed
values some distance apart). But if you know a bit of linear algebra,
you know that eigenvalues are something &lt;em&gt;matrices&lt;/em&gt; have. So what the
hell do I mean, eigenvalues? That’s not a matrix on the left hand side!&lt;/p&gt;

&lt;h1 id=&quot;what-do-you-mean-eigenvalues&quot;&gt;What Do You Mean, Eigenvalues?&lt;/h1&gt;

&lt;p&gt;The bad news is that the theory of &quot;eigenvalues&quot; of operators such as
$-\frac{1}{2}\frac{d^2}{dx^2}$ is pretty complicated. The good news is
that we don’t need it, because we can approximate this operator as a
matrix.&lt;/p&gt;

&lt;p&gt;I can hear you huffing and puffing in indignation. How on earth could a
derivative be a matrix?&lt;/p&gt;

&lt;p&gt;Calm yourself, all will be revealed. Let’s think about what we need to
fit the equation on a computer.&lt;/p&gt;

&lt;p&gt;First of all, we don’t have an infinite amount of memory. Unfortunately,
$\psi (x)$ contains an infinite number of points. That’s not some
special magic of the wave function, it’s true for any function defined
on real numbers – like $f(x) = x^2$. Since there’s an infinite number
of real numbers in any interval, the function is also defined on an
infinite number of points.&lt;/p&gt;

&lt;p&gt;Too bad we don’t have an infinite amount of memory to store this
infinity of numbers. We have to store just a finite number instead. Can
you think of any way to do so?&lt;/p&gt;

&lt;p&gt;One way – the one we’ll follow – is to just divide some finite
interval in to a number of smaller bits, and define just one number as
the value of the function in these bits.&lt;/p&gt;

&lt;p&gt;Confused? Not to worry, here’s what I mean. Let’s suppose we want to
write the function $f(x) = x^2$ in the interval [0,1]. We might divide
this in to a number of smaller intervals:&lt;/p&gt;

\[\begin{aligned}
= [0,0.1]\cup [0.1,0.2]\cup \dots \end{aligned}\]

&lt;p&gt;Then we say, the value in the first small interval is, for example, the average value of the
function at the starting point and end point. So, in the first interval,
it’s $\frac{f(0)+f(0.1)}{2} = \frac{0^2 + 0.1^2}{2} = 0.005$, and so on.
These values then form the discrete version of the function.&lt;/p&gt;

&lt;p&gt;What does this have to do with our Schrödinger equation? Well, look at
the derivative on the left hand side – we have to define it on a finite
set of points instead of a continuous function. Let’s recall the
definition of a derivative:&lt;/p&gt;

\[\begin{aligned}
    \frac{df}{dx} = \lim _{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}.\end{aligned}\]

&lt;p&gt;We now replace the limit $h\rightarrow 0$ with the limit
$h\rightarrow \Delta x$, with $\Delta x$ the length of the small
intervals. For instance, the derivative at the first interval of $x^2$
would be&lt;/p&gt;

\[\begin{aligned}
    \frac{f(0.1)-f(0)}{0.1} = \frac{0.1^2-0^2}{0.1} = 0.1\end{aligned}\]

&lt;p&gt;Let’s see what this looks like graphically. If you want to generate
these plots yourself, it’s actually simple: we can create evenly spaced
values using numpy and then use matplotlib to get the figures. Like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;xcont&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# &quot;Continuous&quot; x
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Discrete approximation
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xsqdisc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Filling the array with the approximation
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;xsqdisc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# outlined in the text
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$x$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$x^2$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xsqdisc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;indigo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xcont&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xcont&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# This creates the silvery vertical lines
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;silver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/xsq.png&quot; alt=&quot;The continuous function $f(x)=x^2$ (the line) and its discrete
approximation (the dots).&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You see that the dots land pretty nicely on the line; perhaps this is a
good approximation to our function! Beware, though – if the function
changes too much within the span of one interval, then we might get a
very poor approximation if our intervals are too large. For example,
look at this approximation of a rapidly oscillating $\sin (x)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/sinus.png&quot; alt=&quot;The continuous function $f(x)=\sin (100x)$ (the line) and its discrete
approximation (the dots).&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not very good, is it? The function oscillates too much between each
interval, so our approximation is way off. Obviously, trying to compute
the derivative using this approximation would be especially useless,
since it would miss all the detail in between each interval! By the way,
you should try to generate that picture yourself as well, as practice.&lt;/p&gt;

&lt;p&gt;So, the key thing is to have a sufficient number of points to guarantee
a good approximation. More points = better, typically.&lt;/p&gt;

&lt;p&gt;What bearing does this have on our Schrödinger equation? Well, every
school boy can now guess that if we represent the wave function as a set
of points:&lt;/p&gt;

\[\begin{aligned}
    \psi (x) \approx \begin{bmatrix}
        \psi (x_1)\\
        \psi (x_2)\\
        \psi (x_3)\\
        \vdots
    \end{bmatrix}\end{aligned}\]

&lt;p&gt;then the discrete approximation of the Hamiltonian must be a
finite-dimensional matrix. So it’s just a normal eigenvalue problem
after all, and it makes a whole lot of sense to talk about eigenvalues.&lt;/p&gt;

&lt;h1 id=&quot;about-that-derivative&quot;&gt;About That Derivative&lt;/h1&gt;

&lt;p&gt;Well, then, what matrix is it? Remember how we defined the derivative on
finite intervals:&lt;/p&gt;

\[\begin{aligned}
    \frac{df}{dx} = \frac{f(x+\Delta x) - f(x)}{\Delta x}\end{aligned}\]

&lt;p&gt;The second derivative approximation (well, one of them) is&lt;/p&gt;

\[\begin{aligned}
    \frac{d^2f}{dx^2} = \frac{f(x+\Delta x)+f(x-\Delta x)-2f(x)}{(\Delta x)^2}\end{aligned}\]

&lt;p&gt;(Can you derive this? Use the backward-difference definition for the
derivative on the first derivative!)&lt;/p&gt;

&lt;p&gt;So how do we find a matrix, say $D$, the effect of which is:&lt;/p&gt;

\[\begin{aligned}
    D\begin{bmatrix}
        \vdots\\
        \psi (x_i)\\
        \vdots
    \end{bmatrix} =\frac{1}{(\Delta x)^2} \begin{bmatrix}
        \vdots\\
        \psi (x_{i+1}) + \psi(x_{i-1}) - 2\psi (x_i)\\
        \vdots
    \end{bmatrix}\end{aligned}\]

&lt;p&gt;At first, this might seem like a daunting task, since we don’t even know how big our matrix needs to be.
Luckily, there’s a simple pattern here which we can exploit. We can
easily discover it by writing the matrix multiplication by components.
If our $\psi$ has $N$ points, then the $i$th element of the
multiplication is&lt;/p&gt;

\[\begin{aligned}
    (D\vec{\psi})_i = \sum _{j=1}^N D_{ij}\psi _j\end{aligned}\]

&lt;p&gt;where the $D_{ij}$ indicates the elements in $i$th row and $j$th column.
Convince yourself that this is indeed the matrix multiplication rule you
learned in linear algebra, if you don’t see it immediately! Try it with
something simple, like a 2-by-2 matrix and a 2-dimensional vector.&lt;/p&gt;

&lt;p&gt;Well, we know what we want on the right hand side. When $j = i$, we want
the coefficient $-2$, so $D_{ii} = -2$. When $j=i\pm 1$, we want the
coefficient $D_{i(i\pm 1)} = 1$. The rest of them must be zero. Then:&lt;/p&gt;

\[\begin{aligned}
    \sum _{j=1}^N D_{ij}\psi _j = D_{ii}\psi _i + D_{i(i+1)}\psi _{i+1} + D_{i(i-1)}\psi _{i-1} = \psi (x_{i+1}) + \psi(x_{i-1}) - 2\psi (x_i)\end{aligned}\]

&lt;p&gt;Then we just need to multiply by $1/(\Delta x) ^2$, which is just a
constant. So we end up with the matrix:&lt;/p&gt;

\[\begin{aligned}
    D = \frac{1}{(\Delta x)^2} \begin{bmatrix}
        -2 &amp;amp; 1 &amp;amp; 0 &amp;amp;  \cdots \\
        1 &amp;amp; -2 &amp;amp; 1 &amp;amp;  \ddots  \\
        0 &amp;amp; 1 &amp;amp; -2 &amp;amp;  \ddots \\
        \vdots &amp;amp; \ddots &amp;amp; \ddots &amp;amp; \ddots 
    \end{bmatrix}\end{aligned}\]

&lt;p&gt;The three central diagonals are non-zero, everything else is zero. Now remember that in the Hamiltonian,
we also have the factor $-\frac{1}{2}$, so we have to multiply that in
as well.&lt;/p&gt;

&lt;p&gt;Let’s build this matrix in Python. Once again, numpy to the rescue.
There’s a function called diagflat that allows us to easily make
matrices from vectors. The second argument indicates how many diagonals
above the central diagonal we want to put our vector. Those are smaller
than the central diagonal, so we need smaller arrays of coefficients.
Like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;D2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagflat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagflat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagflat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s take a step back for a moment. If you’ve ever solved the
Schrödinger equation, you know that you need boundary conditions.
Without them, solving it is like wandering around in an unknown city
with no map – you’re going to wake up in a ditch with a hell of a
hangover. Or something.&lt;/p&gt;

&lt;p&gt;Well, we’ve just constructed a matrix, so what are the boundary
conditions on the $\psi$ given by it? Do the multiplications to examine
the first and last element of the $\psi$-vector after multiplication:&lt;/p&gt;

\[\begin{aligned}
    \psi = \begin{bmatrix}
        \psi _{x_2} - 2\psi _{x_1}\\
        \vdots \\
        \psi _{N-1} - 2\psi _{x_N}
    \end{bmatrix}\end{aligned}\]

&lt;p&gt;See something funny? Only two terms. That’s because our matrix sort of
&quot;cuts off&quot; at the edges. What does this mean? Well, evidently
$\psi_{x_0}$ vanishes from the equation, so it must be 0! We’re so
clever that we’ve taken care of the boundary conditions without even
trying. Three cheers for us!&lt;/p&gt;

&lt;p&gt;Basically, then, this system should depict a particle in a box - a
favorite model of physics teachers everywhere. And fortunately, this
system can be solved analytically. The eigenenergies are&lt;/p&gt;

\[\begin{aligned}
    E_n = \frac{\pi ^2n^2}{2L^2}\end{aligned}\]

&lt;p&gt;For simplicity, why not make our box length $L=1$?&lt;/p&gt;

&lt;p&gt;To recap, taking the matrix we called D2 in Python, we should get as the
first eigenvalue $\pi ^2/2$, the second eigenvalue $\pi ^2\cdot 4 /2$,
and so on. Well, let’s solve them then. Luckily numpy makes this easy
for us. Add this to your code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\ 
 &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see them match very well. But you’ll notice that the higher
eigenvalues are off by larger margin. Why?&lt;/p&gt;

&lt;p&gt;The higher the energy, the more wiggly the wave function is. To see
this, you can plot the wave functions; they’re now stored in the
variable vec so that vec[:,0] is the first eigenvector, vec[:,1] the
second and so on. But the more wiggly the wave function, the worse our
division in to intervals is likely to be, as we saw before withou our
$\sin (x)$ example. So the derivative matrix is going to be a worse
approximation!&lt;/p&gt;

&lt;p&gt;Don’t believe me? Here’s the first few wave functions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/simplyphysics/assets/waves.png&quot; alt=&quot;The first few wave functions of the particle in a box.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;See? More wiggly. More change over one interval. Worse approximation.&lt;/p&gt;

&lt;h1 id=&quot;but-what-about-the-potential&quot;&gt;But What About The Potential?&lt;/h1&gt;

&lt;p&gt;Ah, but we haven’t dealt with the pesky $V(x)$ yet! Is it because it’s
too difficult? No – I’ve left it for last so that you can have an easy
finish. It’s like taking an exam, you don’t want to do the toughest
problem last, when you’re already exhausted. (Incidentally, I almost
always did that, anxious to at least get the easy stuff out of the way.
Then I couldn’t focus on the hard ones. Oh well..)&lt;/p&gt;

&lt;p&gt;What’s the effect of a potential depending on $x$ on $\psi (x)$? Well,
you just multiply $\psi (x)$ at each point $x$. So in matrix terms,&lt;/p&gt;

\[\begin{aligned}
    \hat{V}\vec{\psi} = \begin{bmatrix}
        \vdots \\
        V(x_i)\psi (x_i)\\
        \cdots
    \end{bmatrix}.\end{aligned}\]

&lt;p&gt;A moment’s thought shows that this must be a diagonal matrix. So, for example, 
the harmonic oscillator potential is as simple as:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;xsq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagflat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The total Hamiltonian is then the sum D2+xsq. Make sure this gives the
correct eigenvalues for a harmonic oscillator, too!&lt;/p&gt;

&lt;h1 id=&quot;recap-conclusions-where-to-from-here&quot;&gt;Recap, conclusions, where to from here?&lt;/h1&gt;

&lt;p&gt;So there we go, this computational QM stuff isn’t so hard after all.
Declare yourself the master – nay, the PhD of – nay, the PROFESSOR of
computational quantum mechanics, right?&lt;/p&gt;

&lt;p&gt;Well, not quite. The method we’ve applied here is very naive and not
reliable or workable for complicated systems. Unfortunately the number
of points needed tends to sort of explode in 3 dimensions. This method
is not really used by the professionals.&lt;/p&gt;

&lt;p&gt;What is done instead is that you guess the form of the wave function.
Instead of writing $\psi$ on a finite set of points, you expand it in
terms of some known functions:&lt;/p&gt;

\[\begin{aligned}
    \psi (x) = \sum _i^{N} c_i \phi _i(x).\end{aligned}\]

&lt;p&gt;We choose $\phi _i$ in a clever way so that they’re already pretty close to the
solution. Doing this is as much art as it is science, but even a
relatively bad guess of this form leads to a numerically much easier
problem and thus better results. That’s for another time, though.&lt;/p&gt;

&lt;p&gt;You should use the code we have to explore random potentials you find
interesting. Send me email if you see something funny. I like funny
things.&lt;/p&gt;</content><author><name></name></author><category term="computational quantum mechanics" /><category term="quantum mechanics" /><summary type="html">This little text is intended as a very simple introduction to computational quantum mechanics. There are many books devoted to the topic, filled with one complicated algorithm after the other. Many people devote their whole careers to computational quantum mechanics. It’s harder to find something that allows a total beginner to get started.</summary></entry></feed>